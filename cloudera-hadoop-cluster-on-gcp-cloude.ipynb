{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ Part 1 gcp Instance creation]\n",
    "\n",
    "Follow this google link to create Service account key and select json format.\n",
    "https://cloud.google.com/docs/authentication/getting-started\n",
    "\n",
    "And set this variable\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/path_to_json_file'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install google python api client packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing  gcp libraries and setting up credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from pprint import pprint\n",
    "import googleapiclient.discovery\n",
    "from six.moves import input\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/sathish/gcp_pem.json'\n",
    "compute = googleapiclient.discovery.build('compute', 'v1')\n",
    "project=\"ferrous-weaver-249914\"\n",
    "zone=\"us-east1-b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data nodes and master node definition\n",
    "\n",
    "Refer Google page https://cloud.google.com/vpc/docs/vpc to find out default network internal Ipaddress range for your selected region\n",
    "\n",
    "Ex. us-east1\t10.142.0.0/20\t10.142.0.1\t10.142.0.2 to 10.142.15.253\n",
    "\n",
    "### Select machine types \n",
    "\n",
    "https://cloud.google.com/compute/docs/machine-types\n",
    "\n",
    "n1-standard-1\t1\t3.75\t128\t257\tYes\t2\n",
    "n1-standard-2\t2\t7.50\t128\t257\tYes\t10\n",
    "n1-standard-4\t4\t15\t128\t257\tYes\t10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_size=\"n1-standard-2\"\n",
    "data_node_instances={'datanode1':{'instance_name':'datanode1','hostname':'datanode1.tanu.com','networkIP':'10.142.15.199','script_template':'cloudera_datanode.sh'},\n",
    "                   'datanode2':{'instance_name':'datanode2','hostname':'datanode2.tanu.com','networkIP':'10.142.15.200','script_template':'cloudera_datanode.sh'},\n",
    "                   'datanode3':{'instance_name':'datanode3','hostname':'datanode3.tanu.com','networkIP':'10.142.15.201','script_template':'cloudera_datanode.sh'}}\n",
    "\n",
    "master_node_instances={'masternode1':{'instance_name':'masternode','hostname':'master.tanu.com','networkIP':'10.142.15.198','script_template':'cloudera_manager.sh'}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_data_node_startup_scripts(master_node_name,master_node_ip,script_template):\n",
    "    \n",
    "    script = open('new-client-startup-script.sh', 'w') \n",
    "    for masternode in master_node_instances:\n",
    "        networkIP=master_node_instances[masternode]['networkIP']\n",
    "        hostname=master_node_instances[masternode]['hostname']\n",
    "        print(\"echo {networkIP} {hostname}>>/etc/hosts ##adding Masternode host\".format(networkIP=networkIP,hostname=hostname),file=script)\n",
    "    for datanode in data_node_instances:\n",
    "        networkIP=data_node_instances[datanode]['networkIP']\n",
    "        hostname=data_node_instances[datanode]['hostname']\n",
    "        print(\"echo {networkIP} {hostname}>>/etc/hosts ##adding Data host\".format(networkIP=networkIP,hostname=hostname),file=script)\n",
    "    \n",
    "    print('''\n",
    "    #!/bin/bash\n",
    "\n",
    "    WORK_DIR=\"/var/tmp/startup_dir\"\n",
    "    \n",
    "    mkdir $WORK_DIR\n",
    "    cd $WORK_DIR\n",
    "    hostname=$(hostname -f)\n",
    "    #echo \"{master_node_ip} {master_node_name}\" >>/etc/hosts\n",
    "    yum -y install git-core net-tools krb5-workstation\n",
    "    git clone https://github.com/skumarx87/openldap_MIT-Kerberos_installation.git\n",
    "    cd openldap_MIT-Kerberos_installation\n",
    "    sed -i \"s/idm.tanu.com/{master_node_name}/g\" kerberos_ldap_installation.sh\n",
    "    ./kerberos_ldap_installation.sh client_setup |tee -a kerberos_install.log\n",
    "    sh {script_template} {master_node_name}|tee -a cm_agent_install.log\n",
    "    '''.format(master_node_ip=master_node_ip,master_node_name=master_node_name,script_template=script_template),file=script)\n",
    "\n",
    "    script.close()\n",
    "\n",
    "def generate_master_node_startup_scripts(master_node_name,script_template):\n",
    "    script = open('new-server-startup-script.sh', 'w') \n",
    "    \n",
    "    for masternode in master_node_instances:\n",
    "        networkIP=master_node_instances[masternode]['networkIP']\n",
    "        hostname=master_node_instances[masternode]['hostname']\n",
    "        print(\"echo {networkIP} {hostname}>>/etc/hosts ##adding Masternode host\".format(networkIP=networkIP,hostname=hostname),file=script)\n",
    "    for datanode in data_node_instances:\n",
    "        networkIP=data_node_instances[datanode]['networkIP']\n",
    "        hostname=data_node_instances[datanode]['hostname']\n",
    "        print(\"echo {networkIP} {hostname}>>/etc/hosts ##adding Data host\".format(networkIP=networkIP,hostname=hostname),file=script)\n",
    "    \n",
    "    print('''\n",
    "    #!/bin/bash\n",
    "    WORK_DIR=\"/var/tmp/startup_dir\"\n",
    "    mkdir $WORK_DIR\n",
    "    cd $WORK_DIR\n",
    "    echo \"this is startup script\"\n",
    "    yum -y install git-core net-tools krb5-workstation\n",
    "    git clone https://github.com/skumarx87/openldap_MIT-Kerberos_installation.git\n",
    "    cd openldap_MIT-Kerberos_installation\n",
    "    sed -i \"s/idm.tanu.com/{master_node_name}/g\" kerberos_ldap_installation.sh\n",
    "    ./kerberos_ldap_installation.sh server_setup|tee -a kerberos_install.log\n",
    "    ./kerberos_ldap_installation.sh setup_webserver|tee -a kerberos_install.log\n",
    "    ./kerberos_ldap_installation.sh client_setup|tee -a kerberos_install.log\n",
    "    ./kerberos_ldap_installation.sh create_hadoop_users|tee -a kerberos_install.log\n",
    "    chmod 755 {script_template}\n",
    "    sh {script_template} |tee -a cm_install.log\n",
    "    sh cloudera_datanode.sh {master_node_name} |tee -a cm_agent_install.log\n",
    "\n",
    "    '''.format(master_node_name=master_node_name,script_template=script_template),file=script)\n",
    "    \n",
    "    script.close()\n",
    "    \n",
    "def list_instances():\n",
    "        result = compute.instances().list(project=project, zone=zone).execute()\n",
    "        return result['items'] if 'items' in result else None\n",
    "        # [END list_instances]\n",
    "\n",
    "def list_images():\n",
    "        request = compute.images().list(project=project).execute()\n",
    "        print(request)\n",
    "        #while request is not None:\n",
    "        #       response = request.execute()\n",
    "        #       print(response)\n",
    "                #for image in response['items']:\n",
    "                #       pprint(image)\n",
    "        #       request = compute.images().list_next(previous_request=request, previous_response=response)\n",
    "def list_disks():\n",
    "        request = compute.disks().list(project=project, zone=zone)\n",
    "        while request is not None:\n",
    "            response = request.execute()\n",
    "            for disk in response['items']:\n",
    "                pprint(disk)\n",
    "            request = service.disks().list_next(previous_request=request, previous_response=response)\n",
    "        \n",
    "        \n",
    "def get_instance_networkIP(instance):\n",
    "        request = compute.instances().get(project=project, zone=zone, instance=instance)\n",
    "        response = request.execute()\n",
    "        pprint(response)\n",
    "        host_ip=response['networkInterfaces'][0]['networkIP']\n",
    "        return host_ip\n",
    "def get_instance_networkNatIP(instance):\n",
    "        request = compute.instances().get(project=project, zone=zone, instance=instance)\n",
    "        response = request.execute()\n",
    "        #pprint(response)\n",
    "        nat_ip=response['networkInterfaces'][0]['accessConfigs'][0]['natIP']\n",
    "        return nat_ip\n",
    "def get_instance_metadata_fingerprint(instance):\n",
    "        request = compute.instances().get(project=project, zone=zone, instance=instance)\n",
    "        response = request.execute()\n",
    "        pprint(response)\n",
    "        fingerprint=response['metadata']['fingerprint']\n",
    "        return fingerprint\n",
    "    \n",
    "def delete_instance(instance_name):\n",
    "    request = compute.instances().delete(project=project, zone=zone, instance=instance_name)\n",
    "    response = request.execute()\n",
    "    pprint(response)\n",
    "    \n",
    "def create_instance(name,hostname,startup_script,networkIP):\n",
    "        image_response = compute.images().getFromFamily(project='centos-cloud', family='centos-7').execute()\n",
    "        source_disk_image = image_response['selfLink']\n",
    "        #machine_type = \"zones/%s/machineTypes/n1-standard-1\" % zone\n",
    "        machine_type = \"zones/{zone}/machineTypes/{machine_size}\".format(zone=zone,machine_size=machine_size)\n",
    "        startup_script = open(\n",
    "                os.path.join(\n",
    "                        os.path.abspath(''), startup_script), 'r').read()\n",
    "        config = {\n",
    "            'name': name,\n",
    "            'machineType': machine_type,\n",
    "            'hostname':hostname,\n",
    "            'disks': [\n",
    "                {\n",
    "                    'boot': True,\n",
    "                    'autoDelete': True,\n",
    "                    'initializeParams': {\n",
    "                        'sourceImage': source_disk_image,\n",
    "                        'diskSizeGb': 20,\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            # Specify a network interface with NAT to access the public\n",
    "            # internet.\n",
    "            'networkInterfaces': [{\n",
    "                'network': 'global/networks/default',\n",
    "                'networkIP':networkIP,\n",
    "                'accessConfigs': [\n",
    "                    {'type': 'ONE_TO_ONE_NAT', 'name': 'External NAT'}\n",
    "                ]\n",
    "            }],\n",
    "            'metadata': {\n",
    "            'items': [{\n",
    "                # Startup script is automatically executed by the\n",
    "                # instance upon startup.\n",
    "                'key': 'startup-script',\n",
    "                'value': startup_script\n",
    "            }]\n",
    "            }\n",
    "        }\n",
    "        return compute.instances().insert(project=project,zone=zone,body=config).execute()\n",
    "            \n",
    "def insert_filrewall_rule(name,ports):\n",
    "    firewall_body={\n",
    "        'name':name,\n",
    "        'allowed':[\n",
    "            {\n",
    "                \"IPProtocol\":'tcp',\n",
    "                \"ports\":ports\n",
    "            }\n",
    "        ],\n",
    "        \"sourceRanges\": ['0.0.0.0/0']\n",
    "    }\n",
    "    request = compute.firewalls().insert(project=project, body=firewall_body)\n",
    "    response = request.execute()\n",
    "    pprint(response)\n",
    "    \n",
    "def wait_for_operation(operation):\n",
    "    print('Waiting for operation to finish...')\n",
    "    while True:\n",
    "        result = compute.zoneOperations().get(\n",
    "            project=project,\n",
    "            zone=zone,\n",
    "            operation=operation).execute()\n",
    "\n",
    "        if result['status'] == 'DONE':\n",
    "            print(\"done.\")\n",
    "            if 'error' in result:\n",
    "                raise Exception(result['error'])\n",
    "            return result\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "def Create_hadoop_instaces(wait=True):\n",
    "        #compute = googleapiclient.discovery.build('compute', 'v1')\n",
    "        print(\"Creating Master node\")    \n",
    "                     \n",
    "        for masternode in master_node_instances:\n",
    "            inst_name=master_node_instances[masternode]['instance_name']\n",
    "            host_name=master_node_instances[masternode]['hostname']\n",
    "            networkIP=master_node_instances[masternode]['networkIP']\n",
    "            script_template=master_node_instances[masternode]['script_template']\n",
    "            generate_master_node_startup_scripts(host_name,script_template)\n",
    "            operation=create_instance(inst_name,host_name,'new-server-startup-script.sh',networkIP)\n",
    "            print(operation)\n",
    "            wait_for_operation(operation['name'])\n",
    "\n",
    "        master_node_inst_name=master_node_instances['masternode1']['instance_name']\n",
    "        master_node_host_name=master_node_instances['masternode1']['hostname']\n",
    "        master_node_ip=get_instance_networkIP(master_node_inst_name)\n",
    "        time.sleep(80) ## wait for master node startup script completion\n",
    "        for datanode in data_node_instances:\n",
    "            print(\"Creating Data Node : {}\".format(datanode))\n",
    "            inst_name=data_node_instances[datanode]['instance_name']\n",
    "            host_name=data_node_instances[datanode]['hostname']\n",
    "            networkIP=data_node_instances[datanode]['networkIP']\n",
    "            script_template=data_node_instances[datanode]['script_template']\n",
    "            generate_data_node_startup_scripts(master_node_host_name,master_node_ip,script_template)\n",
    "            operation=create_instance(inst_name,host_name,'new-client-startup-script.sh',networkIP)\n",
    "            wait_for_operation(operation['name'])\n",
    "            \n",
    "\n",
    "def remove_startup_script(wait=True):\n",
    "    \n",
    "    for masternode in master_node_instances:\n",
    "        inst_name=master_node_instances[masternode]['instance_name']\n",
    "        fingerprint_id=get_instance_metadata_fingerprint(inst_name)\n",
    "        metadata_body={'fingerprint': fingerprint_id,\n",
    "              \"items\": [\n",
    "            {\n",
    "              \"key\": \"startup-script\",\n",
    "              \"value\": \"echo startup-script\"\n",
    "            }]\n",
    "            }\n",
    "        request = compute.instances().setMetadata(project=project, zone=zone, instance=inst_name, body=metadata_body)\n",
    "        response = request.execute()\n",
    "        pprint(response)\n",
    "\n",
    "    for datanode in data_node_instances:\n",
    "        inst_name=data_node_instances[datanode]['instance_name']\n",
    "        fingerprint_id=get_instance_metadata_fingerprint(inst_name)\n",
    "        metadata_body={'fingerprint': fingerprint_id,\n",
    "              \"items\": [\n",
    "            {\n",
    "              \"key\": \"startup-script\",\n",
    "              \"value\": \"echo startup-script\"\n",
    "            }]\n",
    "            }\n",
    "        request = compute.instances().setMetadata(project=project, zone=zone, instance=inst_name, body=metadata_body)\n",
    "        response = request.execute()\n",
    "        pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "        Create_hadoop_instaces()\n",
    "        time.sleep(80)## wait for data node startup script completion\n",
    "        remove_startup_script()\n",
    "        #Inserting Firewall rule to allow cloudera manager url port 7180 over the internet\n",
    "        insert_filrewall_rule(\"cloudera-manager-url\",[\"7180\"])\n",
    "        #Inserting Firewall rule to allow Namenode url port 9870 over the internet\n",
    "        insert_filrewall_rule(\"name-node\",[\"9870\"])\n",
    "        #Inserting Firewall rule to allow Yarn resource manager url port 8088 over the internet\n",
    "        insert_filrewall_rule(\"yarn-resource-node\",[\"8088\"])\n",
    "        #Inserting Firewall rule to allow Yarn resource history url port 8088 over the internet\n",
    "        insert_filrewall_rule(\"yarn-history-node\",[\"19888\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ Part 2 Hadoop Cluster Creation]\n",
    "This Section is for creating Hadoop cluster and Cloudera management services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cm_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import cm_client\n",
    "from cm_client.rest import ApiException\n",
    "from pprint import pprint\n",
    "import hashlib\n",
    "import random\n",
    "from html.parser import HTMLParser\n",
    "import urllib.request\n",
    "import urllib\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Cloudera Manager URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_host = 'http://35.185.24.52' ##\n",
    "port = '7180'\n",
    "api_version = 'v30'\n",
    "\n",
    "api_url = api_host + ':' + port + '/api/' + api_version\n",
    "api_client = cm_client.ApiClient(api_url)\n",
    "\n",
    "cm_client.configuration.username = 'admin'\n",
    "cm_client.configuration.password = 'admin'\n",
    "cluster_name='Cluster 8'\n",
    "cdh_version='CDH6'\n",
    "hostname='master.tanu.com'\n",
    "\n",
    "# create an instance of the API class\n",
    "api_client=cm_client.ApiClient(api_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Services Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdhversion='6.3.2'\n",
    "cdh_repo_url = 'https://archive.cloudera.com/cdh6/'\n",
    "## Map the GCP Instance to respective Services like below template ##\n",
    "\n",
    "Server_mapping={'zookeeper':{'hosts':['datanode1.tanu.com','datanode2.tanu.com','datanode3.tanu.com']},\n",
    "               'hdfs':{'NAMENODE':{'hosts':['datanode1.tanu.com']},\n",
    "                      'SECONDARYNAMENODE':{'hosts':['datanode2.tanu.com']},\n",
    "                      'GATEWAY':{'hosts':['master.tanu.com']},\n",
    "                      'DATANODE':{'hosts':['datanode1.tanu.com','datanode2.tanu.com','datanode3.tanu.com']}},\n",
    "               'hive':{'hosts':['datanode1.tanu.com']},\n",
    "               'yarn':{'hosts':['datanode1.tanu.com','datanode2.tanu.com']},\n",
    "               'oozie':{'hosts':['datanode1.tanu.com','datanode2.tanu.com']}\n",
    "              }\n",
    "\n",
    "### List of CLoudera Mangment Services to be created (Note: had issue with NAVIGATORMETADATASERVER)\n",
    "MGMT_SERVICES=[\"EVENTSERVER\", \"HOSTMONITOR\", \"ALERTPUBLISHER\", \"SERVICEMONITOR\",\"ACTIVITYMONITOR\"]\n",
    "\n",
    "#MGMT_SERVICES=[\"NAVIGATORMETADATASERVER\"]\n",
    "\n",
    "AMON_ROLE_CONFIG = {\n",
    "   'firehose_database_host':'master.tanu.com' + \":3306\",\n",
    "   'firehose_database_user':'amon',\n",
    "   'firehose_database_password':'amon',\n",
    "   'firehose_database_type':'mysql',\n",
    "   'firehose_database_name':'amon',\n",
    "   'firehose_heapsize':'314572800'\n",
    "}\n",
    "\n",
    "RMAN_ROLE_CONFIG = {\n",
    "   'headlamp_database_host': 'master.tanu.com'+\":7432\",\n",
    "   'headlamp_database_user': 'rman',\n",
    "   'headlamp_database_password': 'rman',\n",
    "   'headlamp_database_type': 'mysql',\n",
    "   'headlamp_database_name': 'rman',\n",
    "   'headlamp_heapsize': '215964392',\n",
    "}\n",
    "\n",
    "NAV_ROLE_CONFIG = {\n",
    "   'navigator_database_host': 'master.tanu.com'+\":7432\",\n",
    "   'navigator_database_user': 'nav',\n",
    "   'navigator_database_password': 'nav',\n",
    "   'navigator_database_type': 'mysql',\n",
    "   'navigator_database_name': 'nav',\n",
    "   'navigator_heapsize': '215964392',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def manifest_to_dict(manifest_json):\n",
    "    if manifest_json:\n",
    "        dir_list = json.load(\n",
    "            urllib.request.urlopen(manifest_json))['parcels'][0]['parcelName']\n",
    "        parcel_part = re.match(r\"^(.*?)-(.*)-(.*?)$\", dir_list).groups()\n",
    "        return {'product': str(parcel_part[0]).upper(), 'version': str(parcel_part[1]).lower()}\n",
    "    else:\n",
    "        raise Exception(\"Invalid manifest.json\")\n",
    "\n",
    "class LinkParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.links = iter([])\n",
    "\n",
    "    def reset(self):\n",
    "        HTMLParser.reset(self)\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        # Only parse the 'anchor' tag.\n",
    "\n",
    "        if tag == \"a\":\n",
    "            for name, value in attrs:\n",
    "                if '6' in value:\n",
    "                    self.links = chain(self.links, [value.strip('/')])\n",
    "                    \n",
    "def _check_parcel_stage(product, version, expected_stage):\n",
    "    parcel_instance = cm_client.ParcelResourceApi(api_client)\n",
    "    while True:\n",
    "        cdh_parcel = parcel_instance.read_parcel(cluster_name=cluster_name,\n",
    "                                                 product=product, version=version)\n",
    "        if cdh_parcel.stage in expected_stage:\n",
    "            break\n",
    "        if cdh_parcel.state.errors:\n",
    "            raise Exception(str(cdh_parcel.state.errors))\n",
    "        time.sleep(1)\n",
    "                    \n",
    "def parcel_action(product, version, function, expected_stage, action_description):\n",
    "    parcel_instance = cm_client.ParcelResourceApi(api_client)\n",
    "    print(\"%s [%s-%s]\" % (action_description, product, version))\n",
    "    cdh_parcel = parcel_instance\n",
    "    cmd = getattr(cdh_parcel, function)(cluster_name=cluster_name, product=product, version=version)\n",
    "    if not cmd.success:\n",
    "        print(\"ERROR: %s failed!\" % action_description)\n",
    "        exit(0)\n",
    "    return _check_parcel_stage(product, version, expected_stage)\n",
    "\n",
    "def install_parcel():\n",
    "    parser = LinkParser()\n",
    "    parser.feed(str(urllib.request.urlopen(cdh_repo_url).read()))\n",
    "    cdh_version = [x for x in parser.links if cdhversion.strip() in x]\n",
    "    cdh_parcel = manifest_to_dict(\"%s%s/parcels/manifest.json\" % (cdh_repo_url, cdh_version[0]))\n",
    "    # Deploy Parcel > Download> Distribute> Activate\n",
    "    product=cdh_parcel['product']\n",
    "    version=cdh_parcel['version']\n",
    "    parcel_instance = cm_client.ParcelResourceApi(api_client)\n",
    "    try:\n",
    "        # A synchronous command that starts the parcel download.\n",
    "        cluster_parcel = parcel_instance.read_parcel(cluster_name,\n",
    "                                                     product=product,\n",
    "                                                     version=version)\n",
    "        print(\"Parcel current Stage: [%s]\" % cluster_parcel.stage)\n",
    "        if \"ACTIVATED\" not in cluster_parcel.stage:\n",
    "            parcel_action(product=product, version=version, function=\"start_download_command\",\n",
    "                          expected_stage=['DOWNLOADED'],\n",
    "                          action_description=\"Download Parcel\")\n",
    "            parcel_action(product=product, version=version, function=\"start_distribution_command\",\n",
    "                          expected_stage=['DISTRIBUTED'],\n",
    "                          action_description=\"Distribute Parcel\")\n",
    "            parcel_action(product=product, version=version, function=\"activate_command\",\n",
    "                          expected_stage=['ACTIVATED'], action_description=\"Activate Parcel\")\n",
    "        elif 'ACTIVATED' == cluster_parcel.stage:\n",
    "            parcel_action(product=product, version=version,\n",
    "                          function=\"deactivate_command\", expected_stage=['DISTRIBUTED'],\n",
    "                          action_description=\"Deactivate Parcel\")\n",
    "            parcel_action(product=product, version=version,\n",
    "                          function=\"start_removal_of_distribution\", expected_stage=['DOWNLOADED'],\n",
    "                          action_description=\"Un-Distribute Parcel\")\n",
    "    except ApiException as e:\n",
    "        print(\"Exception when calling ParcelResourceApi->start_download_command: %s\\n\" % e)\n",
    "            \n",
    "    \n",
    "    #parcel_instance = cm_client.ParcelResourceApi(api_client)\n",
    "    #print(\"%s [%s-%s]\" % (action_description, product, version))\n",
    "    \n",
    "def create_cluster():\n",
    "    hosts=[]\n",
    "    _hosts = []\n",
    "    [hosts.append(data_node_instances[key]['hostname']) for key in data_node_instances.keys()]\n",
    "    [hosts.append(master_node_instances[key]['hostname']) for key in master_node_instances.keys()]\n",
    "    [_hosts.append({'hostname': host, 'hostId':getattr(get_host_resource(host),'host_id', None)}) for host in hosts]\n",
    "    cluster1={'items':[\n",
    "        {'name': cluster_name,\n",
    "         \"version\": cdh_version,\n",
    "        }\n",
    "        ]}\n",
    "    try:\n",
    "        api_instance = cm_client.ClustersResourceApi(api_client)\n",
    "        api_instance.create_clusters(body=cluster1);\n",
    "        api_instance.add_hosts(cluster_name,body=cm_client.ApiHostRefList(_hosts))\n",
    "    #print(cluster)\n",
    "    #cluster.add_hosts(HOSTS_LIST)\n",
    "    #pprint(api_response)\n",
    "    except ApiException as e:\n",
    "        print(\"Exception when calling ClustersResourceApi->create_clusters: %s\\n\" % e)\n",
    "def get_host_resource(hostname):\n",
    "    api_instance = cm_client.HostsResourceApi(api_client)\n",
    "    #print(api_instance.read_hosts(view='summary'))\n",
    "    try:\n",
    "        api_host_response = [x for x in api_instance.read_hosts(view='summary').items\n",
    "                             if hostname == x.hostname]\n",
    "    except ApiException as e:\n",
    "        print(\"Exception when calling HostsResourceApi->read_hosts: %s\\n\" % e)\n",
    "    #print(api_host_response)\n",
    "    return api_host_response[0]\n",
    "\n",
    "def add_mgmt_service():\n",
    "    amon_role_name = \"ACTIVITYMONITOR\"\n",
    "    service_name = 'mgmt'\n",
    "    service_type = 'MGMT'.upper()\n",
    "    hostname='master.tanu.com'\n",
    "    host_id = getattr(get_host_resource(hostname),'host_id', None)\n",
    "    services_instance = cm_client.MgmtServiceResourceApi(api_client)\n",
    "    mgmt_role_instance = cm_client.MgmtRolesResourceApi(api_client)\n",
    "    rcg_instance = cm_client.MgmtRoleConfigGroupsResourceApi(api_client)\n",
    "    print(mgmt_role_instance)\n",
    "    try:\n",
    "        print(\"Create a CMS service and its associated roles.\")\n",
    "        services_instance.setup_cms(body=cm_client.ApiService(name=service_name,\n",
    "                                                              type=service_type,\n",
    "                                                              display_name='Cloudera Management Service'))\n",
    "        role_list = []\n",
    "        for role_type in MGMT_SERVICES:\n",
    "            role_name = \"%s-%s-%s\" % (service_name, role_type, hashlib.md5(hostname.encode('utf-8')).hexdigest())\n",
    "            #print(host_id)\n",
    "            role_list.append({\"name\": role_name, \"type\": role_type, \"hostRef\": {\"hostId\": host_id}})\n",
    "        #print(role_list)\n",
    "        body = cm_client.ApiRoleList(role_list)\n",
    "        api_response = mgmt_role_instance.create_roles(body=body)\n",
    "        api_response = rcg_instance.read_role_config_groups()\n",
    "        for rcg in api_response.items:\n",
    "            print(rcg)\n",
    "            if rcg.role_type == 'ACTIVITYMONITOR':\n",
    "                activity_config=[]\n",
    "                tmp={activity_config.append({'name':key,'value':value}) for key,value in AMON_ROLE_CONFIG.items()}\n",
    "                print(tmp)\n",
    "                rcg_instance.update_config(rcg.name, message=None,\n",
    "                                           body=cm_client.ApiConfigList(\n",
    "                                               activity_config\n",
    "                                           ))                                       \n",
    "                                            \n",
    "            if rcg.role_type == 'REPORTMANAGER':\n",
    "                rmon_config = []\n",
    "                tmp={rmon_config.append({'name':key,'value':value}) for key,value in RMAN_ROLE_CONFIG.items()}\n",
    "                rcg_instance.update_config(rcg.name, message=None,\n",
    "                                           body=cm_client.ApiConfigList([\n",
    "                                               rmon_config\n",
    "                                            ]))\n",
    "            '''\n",
    "            if rcg.role_type == 'NAVIGATOR':\n",
    "                nav_config = []\n",
    "                tmp={nav_config.append({'name':key,'value':value}) for key,value in NAV_ROLE_CONFIG.items()}\n",
    "                rcg_instance.update_config(rcg.name, message=None,\n",
    "                                           body=cm_client.ApiConfigList([\n",
    "                                               nav_config\n",
    "                                            ]))\n",
    "            '''\n",
    "        services_instance.start_command()\n",
    "    except ApiException as e:\n",
    "        print(\"Exception: %s\\n\" % e)\n",
    "def create_roles(service_name, role_type, hostname):\n",
    "    roles_instance = cm_client.RolesResourceApi(api_client)\n",
    "    host_id = getattr(get_host_resource(hostname), 'host_id', None)\n",
    "    first_part = service_name[:4] + hashlib.md5(service_name).hexdigest()[:8] \\\n",
    "        if len(role_type) > 24 else service_name\n",
    "    role_name = \"-\".join([first_part, role_type, hashlib.md5(hostname.encode('utf-8')).hexdigest()])[:64]\n",
    "    api_response = roles_instance.create_roles(\n",
    "        cluster_name, service_name, body=cm_client.ApiRoleList(\n",
    "            [{'name': role_name,\n",
    "              'type': role_type,\n",
    "              'hostRef': {'hostId': host_id}\n",
    "              }])\n",
    "    )\n",
    "    print(api_response)\n",
    "    return api_response.items[0]\n",
    "\n",
    "def get_service_type(name):\n",
    "    \"\"\"\n",
    "    Returns service based on service type name\n",
    "    :param name:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    services_instance = cm_client.ServicesResourceApi(api_client)\n",
    "    try:\n",
    "        service = [x for x in services_instance.read_services(cluster_name).items if name.upper() == x.type][0]\n",
    "    except IndexError:\n",
    "        service = None\n",
    "\n",
    "    return service\n",
    "\n",
    "\n",
    "def dependencies_for(service_name):\n",
    "    \"\"\"\n",
    "    Utility function returns dict of service dependencies\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    service_config = []\n",
    "    config_types = {\"hue_webhdfs\": ['NAMENODE', 'HTTPFS'], \"hdfs_service\": \"HDFS\", \"sentry_service\": \"SENTRY\",\n",
    "                    \"zookeeper_service\": \"ZOOKEEPER\", \"hbase_service\": \"HBASE\",\n",
    "                    \"hue_hbase_thrift\": \"HBASETHRIFTSERVER\", \"solr_service\": \"SOLR\",\n",
    "                    \"hive_service\": \"HIVE\", \"sqoop_service\": \"SQOOP\",\n",
    "                    \"impala_service\": \"IMPALA\", \"oozie_service\": \"OOZIE\",\n",
    "                    \"mapreduce_yarn_service\": ['MAPREDUCE', 'YARN'], \"yarn_service\": \"YARN\"}\n",
    "\n",
    "    dependency_list = []\n",
    "    # get required service config\n",
    "    services_instance = cm_client.ServicesResourceApi(api_client)\n",
    "    service_conf = services_instance.read_service_config(cluster_name, service_name, view='full')\n",
    "\n",
    "    for name, required in [(config.name, config.required) for config in service_conf.items if config.required]:\n",
    "        if required:\n",
    "            dependency_list.append(name)\n",
    "\n",
    "    service = services_instance.read_service(cluster_name, service_name)\n",
    "    # print service.type\n",
    "\n",
    "    # Extended dependence list, adding the optional ones as well\n",
    "    if service.type == 'HUE':\n",
    "        dependency_list.extend(['hbase_service', 'solr_service', 'sqoop_service',\n",
    "                                'impala_service', 'hue_hbase_thrift', 'hive_service'])\n",
    "    if service.type in ['HIVE', 'HDFS', 'HUE', 'OOZIE', 'MAPREDUCE', 'YARN', 'ACCUMULO16']:\n",
    "        dependency_list.append('zookeeper_service')\n",
    "    if service.type in ['HIVE']:\n",
    "        dependency_list.append('sentry_service')\n",
    "    if service.type == 'OOZIE':\n",
    "        dependency_list.append('hive_service')\n",
    "    if service.type in ['FLUME', 'IMPALA']:\n",
    "        dependency_list.append('hbase_service')\n",
    "    if service.type in ['FLUME', 'SPARK', 'SENTRY', 'ACCUMULO16']:\n",
    "        dependency_list.append('hdfs_service')\n",
    "    if service.type == 'FLUME':\n",
    "        dependency_list.append('solr_service')\n",
    "        \n",
    "    role_instance = cm_client.RolesResourceApi(api_client)\n",
    "    for key in dependency_list:\n",
    "        if key == \"hue_webhdfs\":\n",
    "            service_name = get_service_type('HDFS')\n",
    "\n",
    "            if service_name is not None:\n",
    "                service_roles = role_instance.read_roles(cluster_name, service_name.name,\n",
    "                                                            filter='', view='full')\n",
    "                service_config.append({'name': key,\n",
    "                                        'value': [role for role in service_roles.items\n",
    "                                                    if 'NAMENODE' == role.type][0].name})\n",
    "\n",
    "            if [role for role in service_roles.items if 'HTTPFS' == role.type][0]:\n",
    "                service_config.remove({'name': key,\n",
    "                                        'value': [role for role in service_roles.items\n",
    "                                                    if 'NAMENODE' == role.type][0].name})\n",
    "                service_config.append({'name': key,\n",
    "                                        'value': [role for role in service_roles.items\n",
    "                                                    if 'HTTPFS' == role.type][0].name})\n",
    "\n",
    "        elif key == \"mapreduce_yarn_service\":\n",
    "            for _type in config_types[key]:\n",
    "                if get_service_type(_type) is not None:\n",
    "                    service_config.append({'name': key, 'value': get_service_type(_type).name})\n",
    "                    # prefer YARN over MAPREDUCE\n",
    "                if get_service_type(_type) is not None and _type == 'YARN':\n",
    "                    service_config.remove({'name': key, 'value': get_service_type(_type).name})\n",
    "                    service_config.append({'name': key, 'value': get_service_type(_type).name})\n",
    "\n",
    "        elif key == \"hue_hbase_thrift\":\n",
    "            service_name = get_service_type('HBASE')\n",
    "            if service_name is not None:\n",
    "                service_roles = role_instance.read_roles(cluster_name, service_name.name,\n",
    "                                                            filter='', view='full')\n",
    "                service_config.append({'name': key, 'value':\n",
    "                    [role for role in service_roles.items if config_types[key] == role.type][0].name})\n",
    "        else:\n",
    "            if get_service_type(config_types[key]) is not None:\n",
    "                service_config.append({'name': key, 'value': get_service_type(config_types[key]).name})\n",
    "\n",
    "    return service_config\n",
    "\n",
    "\n",
    "def add_zookeeper():\n",
    "    service_name = 'zookeeper'\n",
    "    servers_list=Server_mapping[service_name]['hosts']\n",
    "    service_type = 'ZOOKEEPER'.upper()\n",
    "    #for hostname in random.sample(servers_list, 3 if len(servers_list) >= 3 else 1):\n",
    "    #    print(hostname)\n",
    "    services_instance = cm_client.ServicesResourceApi(api_client)\n",
    "    rcg_instance = cm_client.RoleConfigGroupsResourceApi(api_client)\n",
    "    \n",
    "    try:\n",
    "        print(\"Create a %s service and its associated roles.\" % service_name)\n",
    "        services_instance.create_services(cluster_name,\n",
    "                                          body=cm_client.ApiServiceList(\n",
    "                                              [{'name': service_name,\n",
    "                                                'type': service_type}]))\n",
    "        services_instance.update_service_config(cluster_name, service_name, message=None,\n",
    "                                                body=cm_client.ApiServiceConfig([\n",
    "                                                    {'name': 'zookeeper_datadir_autocreate', 'value': False},\n",
    "                                                    ]))\n",
    "        # Updates the config for the given role config group.\n",
    "        api_response = rcg_instance.read_role_config_groups(cluster_name, service_name)\n",
    "        for rcg in api_response.items:\n",
    "            if rcg.role_type == 'SERVER':\n",
    "                rcg_instance.update_config(\n",
    "                    cluster_name, rcg.name, service_name, message=None,\n",
    "                    body=cm_client.ApiConfigList([\n",
    "                        {'name': 'maxClientCnxns', 'value': '1024'},\n",
    "                        {'name': 'zookeeper_server_java_heapsize', 'value': '492830720'},\n",
    "                        {'name': 'maxSessionTimeout', 'value': '60000'}\n",
    "                    ])\n",
    "                )\n",
    "                # Create new roles in a given service.\n",
    "                for hostname in random.sample(servers_list, 3 if len(servers_list) >= 3 else 1):\n",
    "                    role = create_roles(service_name=service_name, role_type=rcg.role_type,hostname=hostname)\n",
    "                                    \n",
    "    except ApiException as e:\n",
    "        print(\"Exception: %s\\n\" % e)\n",
    "\n",
    "def add_hdfs():\n",
    "    service_name = 'hdfs'\n",
    "    #servers_list=Server_mapping[service_name]['hosts']\n",
    "    service_type = 'HDFS'.upper()\n",
    "    #for hostname in random.sample(servers_list, 3 if len(servers_list) >= 3 else 1):\n",
    "    #    print(hostname)\n",
    "    services_instance = cm_client.ServicesResourceApi(api_client)\n",
    "    rcg_instance = cm_client.RoleConfigGroupsResourceApi(api_client)\n",
    "    \n",
    "    try:\n",
    "        print(\"Create a %s service and its associated roles.\" % service_name)\n",
    "        services_instance.create_services(cluster_name,\n",
    "                                          body=cm_client.ApiServiceList(\n",
    "                                              [{'name': service_name,\n",
    "                                                'type': service_type}]))\n",
    "        service_config = [{'name': 'hdfs_verify_ec_with_topology_enabled', 'value': 'false'},\n",
    "                          {'name': 'dfs_replication', 'value': '1'},]\n",
    "        for kv in dependencies_for(service_name=service_name):\n",
    "            service_config.append(kv)\n",
    "        # Update hbase service dependency\n",
    "        services_instance.update_service_config(cluster_name, service_name, message=None,\n",
    "                                                body=cm_client.ApiServiceConfig(service_config))\n",
    "        # Updates the config for the given role config group.\n",
    "        api_response = rcg_instance.read_role_config_groups(cluster_name, service_name)\n",
    "        for rcg in api_response.items:\n",
    "            try:\n",
    "                servers_list=Server_mapping[service_name][rcg.role_type]['hosts']\n",
    "            except KeyError:\n",
    "                print(\"Key Error ?????????? {}\".format(rcg.role_type))\n",
    "                continue\n",
    "            print(\"servers list !!!!!!!!!!!!!!\")\n",
    "            print(servers_list)\n",
    "         \n",
    "            if rcg.role_type == 'NAMENODE':\n",
    "                rcg_instance.update_config(\n",
    "                    cluster_name, rcg.name, service_name, message=None,\n",
    "                    body=cm_client.ApiConfigList([\n",
    "                        {'name': 'dfs_name_dir_list', 'value': '/data/dfs/nn'},\n",
    "                        {'name': 'dfs_namenode_handler_count', 'value': '30'},\n",
    "                        {'name': 'dfs_namenode_service_handler_count', 'value': '30'},\n",
    "                        {'name': 'dfs_namenode_servicerpc_address', 'value': '8022'}\n",
    "                    ])\n",
    "                )\n",
    "                    # Create new roles in a given service.\n",
    "                for hostname in servers_list:\n",
    "                    role = create_roles(service_name=service_name,\n",
    "                                        role_type=rcg.role_type,\n",
    "                                        hostname=hostname)\n",
    "                    \n",
    "            if rcg.role_type == 'SECONDARYNAMENODE':\n",
    "                rcg_instance.update_config(\n",
    "                    cluster_name, rcg.name, service_name, message=None,\n",
    "                    body=cm_client.ApiConfigList([\n",
    "                        {'name': 'fs_checkpoint_dir_list', 'value': '/data/dfs/snn'},\n",
    "                    ])\n",
    "                )\n",
    "                    # Create new roles in a given service.\n",
    "                for hostname in servers_list:\n",
    "                    role = create_roles(service_name=service_name,\n",
    "                                        role_type=rcg.role_type,\n",
    "                                        hostname=hostname)\n",
    "                    \n",
    "            if rcg.role_type == 'DATANODE':\n",
    "                rcg_instance.update_config(\n",
    "                    cluster_name, rcg.name, service_name, message=None,\n",
    "                    body=cm_client.ApiConfigList([\n",
    "                        {'name': 'datanode_java_heapsize', 'value': '127926272'},\n",
    "                        {'name': 'dfs_data_dir_list', 'value': '/data/dfs/dn'},\n",
    "                        {'name': 'dfs_datanode_data_dir_perm', 'value': '755'},\n",
    "                        {'name': 'dfs_datanode_du_reserved', 'value': '3218866585'},\n",
    "                        {'name': 'dfs_datanode_failed_volumes_tolerated', 'value': '0'},\n",
    "                        {'name': 'dfs_datanode_max_locked_memory', 'value': '316669952'},\n",
    "                    ])\n",
    "                )\n",
    "                    # Create new roles in a given service.\n",
    "                for hostname in servers_list:\n",
    "                    role = create_roles(service_name=service_name,\n",
    "                                        role_type=rcg.role_type,\n",
    "                                        hostname=hostname)\n",
    "    except ApiException as e:\n",
    "        print(\"Exception: %s\\n\" % e)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    create_cluster();\n",
    "    add_mgmt_service()\n",
    "    install_parcel()\n",
    "    add_zookeeper()\n",
    "    add_hdfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment to delete all hadoop compute instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cm_client.ApiConfigList([RMAN_ROLE_CONFIG])\n",
    "import json\n",
    "r=json.dumps(AMON_ROLE_CONFIG)\n",
    "loaded_r = json.loads(r)\n",
    "#q=[]\n",
    "#for key in AMON_ROLE_CONFIG:\n",
    "#    q.append({\"name\":key,\"value\":AMON_ROLE_CONFIG[key]})\n",
    "#print(json.dumps(q))\n",
    "\n",
    "#x={key: value for key, value in AMON_ROLE_CONFIG}\n",
    "#values = map(lambda key: AMON_ROLE_CONFIG[key], AMON_ROLE_CONFIG.keys())\n",
    "#print(values)\n",
    "u=[]\n",
    "x={u.append({'name':key,'value':i}) for key,i in AMON_ROLE_CONFIG.items()}\n",
    "print(json.dumps(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_hadoop_instaces(wait=True):\n",
    "    for masternode in master_node_instances:\n",
    "        inst_name=master_node_instances[masternode]['instance_name']\n",
    "        delete_instance(inst_name)\n",
    "\n",
    "    for datanode in data_node_instances:\n",
    "        inst_name=data_node_instances[datanode]['instance_name']\n",
    "        delete_instance(inst_name)\n",
    "\n",
    "        \n",
    "delete_hadoop_instaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_host_entry_details():\n",
    "    for masternode in master_node_instances:\n",
    "        inst_name=master_node_instances[masternode]['instance_name']\n",
    "        hostname=master_node_instances[masternode]['hostname']\n",
    "        nat_ip=get_instance_networkNatIP(inst_name)\n",
    "        print(\"{nat_ip} {hostname}\".format(nat_ip=nat_ip,hostname=hostname))\n",
    "\n",
    "    for datanode in data_node_instances:\n",
    "        inst_name=data_node_instances[datanode]['instance_name']\n",
    "        hostname=data_node_instances[datanode]['hostname']\n",
    "        nat_ip=get_instance_networkNatIP(inst_name)\n",
    "        print(\"{nat_ip} {hostname}\".format(nat_ip=nat_ip,hostname=hostname))\n",
    "\n",
    "get_host_entry_details()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
